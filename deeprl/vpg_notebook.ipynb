{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, output_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        return self.softmax(self.fc2(x))\n",
    "    def draw_action(self, x):\n",
    "        action_prob = self.forward(x)\n",
    "        m = torch.distributions.Categorical(action_prob)\n",
    "        action = m.sample()\n",
    "        log_probs = m.log_prob(action)\n",
    "        return action.item(), log_probs\n",
    "    \n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 1)\n",
    "    def forward(self,x):\n",
    "        return self.fc2(torch.nn.functional.relu(self.fc1(x)))\n",
    "    \n",
    "def collect_trajectory():\n",
    "    state_list = []; action_list = []; reward_list = []; log_prob_list = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps < max_num_steps_per_ep:\n",
    "        action, log_prob = policy.draw_action(state)\n",
    "        newstate, reward, done, _ = env.step(action)\n",
    "        state_list.append(state); action_list.append(action)\n",
    "        reward_list.append(reward); log_prob_list.append(log_prob)\n",
    "        steps += 1  \n",
    "        state = newstate\n",
    "    return state_list, action_list, reward_list, log_prob_list\n",
    "    \n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "policy = PolicyNet(input_size,output_size)\n",
    "value = ValueNet(input_size)\n",
    "num_iter = 1; num_ep_per_iter = 20; num_traj_per_iter = 10; \n",
    "max_num_steps_per_ep = 200; gamma = 0.99\n",
    "\n",
    "for i in range(num_iter):\n",
    "    state_list, action_list, reward_list = collect_trajectory()\n",
    "    # Compute rewards-to-go Rt\n",
    "    # Compute advantage A\n",
    "    # Estimate policy gradient\n",
    "    # GD step on policy\n",
    "    # Estimate value function gradient \n",
    "    # GD step on value function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.00642266, 0.00365945, 0.01782137, 0.04382058]),\n",
       "  array([ 0.00649585, -0.19171345,  0.01869778,  0.34207261]),\n",
       "  array([0.00266158, 0.00313756, 0.02553924, 0.05534401]),\n",
       "  array([ 0.00272433, -0.1923411 ,  0.02664612,  0.3559741 ]),\n",
       "  array([-0.00112249, -0.38783157,  0.0337656 ,  0.65693874]),\n",
       "  array([-0.00887912, -0.19319551,  0.04690437,  0.37507627]),\n",
       "  array([-0.01274303,  0.00122991,  0.0544059 ,  0.09754372]),\n",
       "  array([-0.01271844,  0.1955316 ,  0.05635677, -0.17749014]),\n",
       "  array([-0.0088078 ,  0.38980365,  0.05280697, -0.45187545]),\n",
       "  array([-0.00101173,  0.58414058,  0.04376946, -0.72745646]),\n",
       "  array([ 0.01067108,  0.38844172,  0.02922033, -0.42132519]),\n",
       "  array([ 0.01843991,  0.19291821,  0.02079383, -0.11957558]),\n",
       "  array([ 0.02229828, -0.00249539,  0.01840232,  0.17959441]),\n",
       "  array([ 0.02224837,  0.19235845,  0.0219942 , -0.1072269 ]),\n",
       "  array([ 0.02609554,  0.38715843,  0.01984967, -0.39289042]),\n",
       "  array([ 0.03383871,  0.1917605 ,  0.01199186, -0.09401585]),\n",
       "  array([ 0.03767392,  0.38670854,  0.01011154, -0.38289136]),\n",
       "  array([ 0.04540809,  0.58168548,  0.00245371, -0.67236906]),\n",
       "  array([ 0.0570418 ,  0.77677323, -0.01099367, -0.96427843]),\n",
       "  array([ 0.07257726,  0.58180069, -0.03027924, -0.67506935]),\n",
       "  array([ 0.08421328,  0.77733005, -0.04378062, -0.97712969]),\n",
       "  array([ 0.09975988,  0.58282167, -0.06332322, -0.69851408]),\n",
       "  array([ 0.11141631,  0.38863221, -0.0772935 , -0.4264183 ]),\n",
       "  array([ 0.11918896,  0.58475899, -0.08582186, -0.74243178]),\n",
       "  array([ 0.13088414,  0.39091993, -0.1006705 , -0.47794398]),\n",
       "  array([ 0.13870253,  0.58730837, -0.11022938, -0.80058233]),\n",
       "  array([ 0.1504487 ,  0.78375567, -0.12624103, -1.12580673]),\n",
       "  array([ 0.16612382,  0.98028528, -0.14875716, -1.45527158]),\n",
       "  array([ 0.18572952,  0.78726939, -0.17786259, -1.21251873]),\n",
       "  array([ 0.20147491,  0.59483179, -0.20211297, -0.98043247])],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
